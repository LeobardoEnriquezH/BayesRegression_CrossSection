geom_linerange(ymin = 0) +
labs(y = "Probability")
outliers_df %>%
filter(probability > 0.50)
# Type your code for Question 5 here.
case_1<-outliers_df %>%
filter(probability > 0.50)
case_1$probability>0.99
# prob of a case being an outlier:
#   being below or above 3 standard deviations from 0
(prob_outlier <- pnorm(-3) + pnorm(3, lower.tail = FALSE))
# probability of a signle case not being an outler is therefore the complement
(prob_not_outlier <- 1 - prob_outlier)
# probability of no outliers in the sample of n assuming errors are independent a priori
n <- nrow(wage)
(prob_no_outliers <- prob_not_outlier^n)
# probability of at least one outlier in the sample is the complement of the
# probability of no outliers in the sample of n
1 - prob_no_outliers
n <- nrow(wage)
(prob_obs_not_outlier <- 0.95^(1/n))
(newk <- qnorm(0.5 + 0.5 * prob_obs_not_outlier))
outliers <- Bayes.outlier(m_lwage_iq, prior.prob=0.95)
# Type your code for Qustion 6 here.
#Case 784
outliers$prob.outlier>outliers$prior.prob
m_lwage_full <- lm(lwage ~ . - wage, data = wage)
# Type your code for Question 7 here.
summary(m_lwage_full)
#married1=0.20076, black1=-0.10514. So, married black man=0.20076-0.10514=0.09562
#and single non-black man =-0.20076+0.10514 = -0.09562
BIC(m_lwage_full)
m_lwage_nobrthord <- lm(lwage ~ . - wage - brthord, data = na.omit(wage))
BIC(m_lwage_nobrthord)
# Type your code for Question 8 here.
m_lwage_nosibs <- lm(lwage ~ . - wage - sibs, data = na.omit(wage))
BIC(m_lwage_nosibs)
m_lwage_nofeduc <- lm(lwage ~ . - wage - feduc, data = na.omit(wage))
BIC(m_lwage_nofeduc)
m_lwage_nomeduc <- lm(lwage ~ . - wage - meduc, data = na.omit(wage))
BIC(m_lwage_nomeduc)
# Type your code for Exercise 4 here.
wage_naomit <- na.omit(wage)
n<- nrow(wage_naomit)
m_lwage_full <- lm(lwage ~ . - wage, data = wage_naomit)
stepAIC(m_lwage_full, trace = 1, keep = NULL, steps = 1000, direction ="backward", k = log(n), data = na.omit(wage) )
# Exclude observations with missing values in the data set
wage_no_na <- na.omit(wage)
# Fit the model using Bayesian linear regression, `bas.lm` function in the `BAS` package
bma_lwage <- bas.lm(lwage ~ . -wage, data = wage_no_na,
prior = "BIC",
modelprior = uniform())
# Print out the marginal posterior inclusion probabilities for each variable
bma_lwage
# Top 5 most probably models
summary(bma_lwage)
# Obtain the coefficients from the model `bma_lwage`
coef_lwage <- coefficients(bma_lwage)
# `iq` is the 3rd variable, while `sibs` is the 13th variable in the data set
plot(coef_lwage, subset = c(3,13), ask = FALSE)
confint(coef_lwage)
#wage_red <- wage %>% select(-c("wage", "sibs", "brthord", "meduc", "feduc"))
wage_red = subset(wage, select = -c(wage, sibs, brthord, meduc, feduc))
bma_lwage_red <- bas.lm(lwage ~ ., data = wage_red,
prior = "ZS-null",
modelprior = uniform())
# Type your code for Question 9 here.
bma_lwage_red
# Type your code for Question 10 here.
summary(bma_lwage)
# Type your code for Exercise 5 here.
coef_lwage_red <- coefficients(bma_lwage_red)
# `iq` is the 3rd variable, while `sibs` is the 13th variable in the data set
plot(coef_lwage_red, subset = c(8), ask = FALSE)
coef(bma_lwage_red)
coef(bma_lwage_red) %>%
confint()
ci_urban <- coef(bma_lwage_red) %>%
confint(parm = "urban1") %>%
exp()
ci_urban
(ci_urban - 1) * 100
# Type your code for Exercise 6 here
ci_educ <- coef(bma_lwage_red) %>%
confint(parm = "educ") %>%
exp()
ci_educ
(ci_educ - 1) * 100
BPM_pred_lwage <- predict(bma_lwage, estimator = "BPM", se.fit = TRUE)
variable.names(BPM_pred_lwage)
HPM_pred_lwage <- predict(bma_lwage, estimator = "HPM")
variable.names(HPM_pred_lwage)
MPM_pred_lwage <- predict(bma_lwage, estimator = "MPM")
variable.names(MPM_pred_lwage)
# Type your code for Question 11 here.
# Find the index of observation with the largest fitted value
opt <- which.max(BPM_pred_lwage$fit)
# Extract the row with this observation and glimpse at the row
wage_no_na %>%
slice(opt) %>%
glimpse()
ci_lwage <- confint(BPM_pred_lwage, parm = "pred")
ci_lwage[opt,]
exp(ci_lwage[opt,])
BMA_pred_lwage <- predict(bma_lwage, estimator = "BMA", se.fit = TRUE)
ci_bma_lwage <- confint(BMA_pred_lwage, estimator = "BMA")
opt_bma <- which.max(BMA_pred_lwage$fit)
exp(ci_bma_lwage[opt_bma, ])
# Type your code for Question 12 here.
BPM_pred_lwage <- predict(bma_lwage, estimator = "BPM", se.fit = TRUE)
ci_bpm_lwage <- confint(BPM_pred_lwage, estimator = "BPM")
opt_bpm <- which.max(BPM_pred_lwage$fit)
exp(ci_bpm_lwage[opt_bpm, ])
View(wage)
library(tidyverse)
library(statsr)
data(brfss)
credible_interval_app()
# Type your code for Exercise 1 here.
qnorm(c(0.025, 0.975), mean = 10, sd = 2.236)
# Type your code for the Exercise 2 here.
qbeta(c(0.05, 0.95), shape1 = 2, shape2 = 5)
# Type your code for the Exercise 3 here.
qgamma(c(0.005, 0.995), shape = 4, rate = 8)
table(brfss$sex)
n <- length(brfss$sex)
x <- sum(brfss$sex == "Female")
# Type your code for Question 7 here.
table(brfss$exercise)
# Type your code for Question 10 here.
# Type your code for Question 12 here.
# Type your code for the Exercise 6 here.
knitr::opts_chunk$set(echo = TRUE)
library(BAS)
data(bodyfat)
summary(bodyfat)
# Frequentist OLS linear regression
bodyfat.lm = lm(Bodyfat ~ Abdomen, data = bodyfat)
summary(bodyfat.lm)
# Extract coefficients
beta = coef(bodyfat.lm)
# Visualize regression line on the scatter plot
library(ggplot2)
ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) +
geom_point(color = "blue") +
geom_abline(intercept = beta[1], slope = beta[2], size = 1) +
xlab("abdomen circumference (cm)")
# Obtain residuals and n
resid = residuals(bodyfat.lm)
n = length(resid)
# Calculate MSE
MSE = 1/ (n - 2) * sum((resid ^ 2))
MSE
# Combine residuals and fitted values into a data frame
result = data.frame(fitted_values = fitted.values(bodyfat.lm),
residuals = residuals(bodyfat.lm))
# Load library and plot residuals versus fitted values
library(ggplot2)
ggplot(data = result, aes(x = fitted_values, y = residuals)) +
geom_point(pch = 1, size = 2) +
geom_abline(intercept = 0, slope = 0) +
xlab(expression(paste("fitted value ", widehat(Bodyfat)))) +
ylab("residuals")
# Find the observation with the largest fitted value
which.max(as.vector(fitted.values(bodyfat.lm)))
# Shows this observation has the largest Abdomen
which.max(bodyfat$Abdomen)
plot(bodyfat.lm, which = 2)
output = summary(bodyfat.lm)$coef[, 1:2]
output
out = cbind(output, confint(bodyfat.lm))
colnames(out) = c("posterior mean", "posterior std", "2.5", "97.5")
round(out, 2)
library(ggplot2)
# Construct current prediction
alpha = bodyfat.lm$coefficients[1]
beta = bodyfat.lm$coefficients[2]
new_x = seq(min(bodyfat$Abdomen), max(bodyfat$Abdomen),
length.out = 100)
y_hat = alpha + beta * new_x
# Get lower and upper bounds for mean
ymean = data.frame(predict(bodyfat.lm,
newdata = data.frame(Abdomen = new_x),
interval = "confidence",
level = 0.95))
# Get lower and upper bounds for prediction
ypred = data.frame(predict(bodyfat.lm,
newdata = data.frame(Abdomen = new_x),
interval = "prediction",
level = 0.95))
output = data.frame(x = new_x, y_hat = y_hat, ymean_lwr = ymean$lwr, ymean_upr = ymean$upr,
ypred_lwr = ypred$lwr, ypred_upr = ypred$upr)
# Extract potential outlier data point
outlier = data.frame(x = bodyfat$Abdomen[39], y = bodyfat$Bodyfat[39])
# Scatter plot of original
plot1 = ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) + geom_point(color = "blue")
# Add bounds of mean and prediction
plot2 = plot1 +
geom_line(data = output, aes(x = new_x, y = y_hat, color = "first"), lty = 1) +
geom_line(data = output, aes(x = new_x, y = ymean_lwr, lty = "second")) +
geom_line(data = output, aes(x = new_x, y = ymean_upr, lty = "second")) +
geom_line(data = output, aes(x = new_x, y = ypred_upr, lty = "third")) +
geom_line(data = output, aes(x = new_x, y = ypred_lwr, lty = "third")) +
scale_colour_manual(values = c("orange"), labels = "Posterior mean", name = "") +
scale_linetype_manual(values = c(5, 3), labels = c( "95% CI for mean", "95% CI for predictions")
, name = "") +
theme_bw() +
theme(legend.position = c(1, 0), legend.justification = c(1.5, 0))
# Identify potential outlier
plot2 + geom_point(data = outlier, aes(x = x, y = y), color = "orange", pch = 1, cex = 6)
pred.39 = predict(bodyfat.lm, newdata = bodyfat[39, ], interval = "prediction", level = 0.95)
out = cbind(bodyfat[39,]$Abdomen, pred.39)
colnames(out) = c("abdomen", "prediction", "lower", "upper")
out
outliers = Bayes.outlier(bodyfat.lm, k=3)
# Extract the probability that Case 39 is an outlier
prob.39 = outliers$prob.outlier[39]
prob.39
n = nrow(bodyfat)
# probability of no outliers if outliers have errors greater than 3 standard deviation
prob = (1 - (2 * pnorm(-3))) ^ n
prob
# probability of at least one outlier
prob.least1 = 1 - (1 - (2 * pnorm(-3))) ^ n
prob.least1
#Value of K for the prior probability of no outliers is 0.95
new_k = qnorm(0.5 + 0.5 * 0.95 ^ (1 / n))
new_k
# Calculate probability of being outliers using new k value
outliers.new = Bayes.outlier(bodyfat.lm, k = new_k)
# Extract the probability of Case 39
prob.new.39 = outliers.new$prob.outlier[39]
prob.new.39
2 * pnorm(-new_k)
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
summary(cognitive)
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs = as.numeric(cognitive$mom_hs > 0)
# Modify column names of the data set
colnames(cognitive) = c("kid_score", "hs", "IQ", "work", "age")
# Import library
library(BAS)
# Use bas.lm to run regression model
cog.bas = bas.lm(kid_score ~ ., data = cognitive, prior = "BIC",
modelprior = Bernoulli(1),
include.always = ~ .,
n.models = 1)
cog.coef = coef(cog.bas)
cog.coef
par(mfrow = c(2, 2), col.lab = "darkgrey", col.axis = "darkgrey", col = "darkgrey")
plot(cog.coef, subset = 2:5, ask = F)
confint(cog.coef, parm = 2:5)
out = confint(cog.coef)[, 1:2]
# Extract the upper and lower bounds of the credible intervals
names = c("posterior mean", "posterior std", colnames(out))
out = cbind(cog.coef$postmean, cog.coef$postsd, out)
colnames(out) = names
round(out, 2)
# Load the library in order to read in data from website
library(foreign)
# Read in cognitive score data set and process data tranformations
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","IQ", "work", "age")
# Compute the total number of observations
n = nrow(cognitive)
# Full model using all predictors
cog.lm = lm(kid_score ~ ., data=cognitive)
# Perform BIC elimination from full model
# k = log(n): penalty for BIC rather than AIC
cog.step = step(cog.lm, k=log(n))
# Import library
library(BAS)
# Use `bas.lm` to run regression model
cog.BIC = bas.lm(kid_score ~ ., data = cognitive,
prior = "BIC", modelprior = uniform())
cog.BIC
# Find the index of the model with the largest logmarg
best = which.max(cog.BIC$logmarg)
# Retrieve the index of variables in the best model, with 0 as the index of the intercept
bestmodel = cog.BIC$which[[best]]
bestmodel
# Create an indicator vector indicating which variables are used in the best model
# First, create a 0 vector with the same dimension of the number of variables in the full model
bestgamma = rep(0, cog.BIC$n.vars)
# Change the indicator to 1 where variables are used
bestgamma[bestmodel + 1] = 1
bestgamma
# Fit the best BIC model by imposing which variables to be used using the indicators
cog.bestBIC = bas.lm(kid_score ~ ., data = cognitive,
prior = "BIC", n.models = 1,  # We only fit 1 model
bestmodel = bestgamma,  # We use bestgamma to indicate variables
modelprior = uniform())
# Retrieve coefficients information
cog.coef = coef(cog.bestBIC)
# Retrieve bounds of credible intervals
out = confint(cog.coef)[, 1:2]
# Combine results and construct summary table
coef.BIC = cbind(cog.coef$postmean, cog.coef$postsd, out)
names = c("post mean", "post sd", colnames(out))
colnames(coef.BIC) = names
coef.BIC
# Import libary
library(BAS)
# Use `bas.lm` for regression
cog_bas = bas.lm(kid_score ~ hs + IQ + work + age,
data = cognitive, prior = "BIC",
modelprior = uniform())
names(cog_bas)
round(summary(cog_bas), 3)
print(cog_bas)
image(cog_bas, rotate = F)
cog_coef = coef(cog_bas)
cog_coef
#par(mfrow = c(2, 2))
#plot(cog_coef, subset = c(2:5))
# Data processing
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","IQ", "work", "age")
# Run regression
library(BAS)
cog_bas = bas.lm(kid_score ~ hs + IQ + work + age,
prior = "BIC",
modelprior = uniform(),
data = cognitive)
library(ggplot2)
# Construct data frame for plotting
output = data.frame(model.size = cog_bas$size, model.prob = cog_bas$postprobs)
# Plot model size vs mode posterior probability
ggplot(data = output, aes(x = model.size, y = model.prob)) +
geom_point(color = "blue", pch = 17, size = 3) +
xlab("model size") + ylab("model posterior probability")
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","IQ", "work", "age")
# Extract size of data set
n = nrow(cognitive)
library(BAS)
# Unit information prior
cog.g = bas.lm(kid_score ~ ., data=cognitive, prior="g-prior",
a=n, modelprior=uniform())
# a is the hyperparameter in this case g=n
# Zellner-Siow prior with Jeffrey's reference prior on sigma^2
cog.ZS = bas.lm(kid_score ~ ., data=cognitive, prior="JZS",
modelprior=uniform())
# Hyper g/n prior
cog.HG = bas.lm(kid_score ~ ., data=cognitive, prior="hyper-g-n",
a=3, modelprior=uniform())
# hyperparameter a=3
# Empirical Bayesian estimation under maximum marginal likelihood
cog.EB = bas.lm(kid_score ~ ., data=cognitive, prior="EB-local",
a=n, modelprior=uniform())
# BIC to approximate reference prior
cog.BIC = bas.lm(kid_score ~ ., data=cognitive, prior="BIC",
modelprior=uniform())
# AIC
cog.AIC = bas.lm(kid_score ~ ., data=cognitive, prior="AIC",
modelprior=uniform())
probne0 = cbind(cog.BIC$probne0, cog.g$probne0, cog.ZS$probne0, cog.HG$probne0,
cog.EB$probne0, cog.AIC$probne0)
colnames(probne0) = c("BIC", "g", "ZS", "HG", "EB", "AIC")
rownames(probne0) = c(cog.BIC$namesx)
library(ggplot2)
# Generate plot for each variable and save in a list
P = list()
for (i in 2:5){
mydata = data.frame(prior = colnames(probne0), posterior = probne0[i, ])
mydata$prior = factor(mydata$prior, levels = colnames(probne0))
p = ggplot(mydata, aes(x = prior, y = posterior)) +
geom_bar(stat = "identity", fill = "blue") + xlab("") +
ylab("") +
ggtitle(cog.g$namesx[i])
P = c(P, list(p))
}
library(cowplot)
do.call(plot_grid, c(P))
# Extract coefficients
beta = coef(bodyfat.lm)
# Visualize regression line on the scatter plot
library(ggplot2)
ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) +
geom_point(color = "blue") +
geom_abline(intercept = beta[1], slope = beta[2], linewidth = 1) +
xlab("abdomen circumference (cm)")
knitr::opts_chunk$set(echo = TRUE)
library(PairedData)
library(tidyverse)
library(statsr)
library(dplyr)
library(ggplot2)
theta=seq(from=0,to=1,by=.01)
plot(theta,dbeta(theta,1,1),type="l")
plot(theta,dbeta(theta,4,2),type="l")
plot(theta,dbeta(theta,8,4),type="l")
1-pbeta(.25,8,4)
1-pbeta(.5,8,4)
1-pbeta(.8,8,4)
41/(41+11)  # posterior mean
33/40       # MLE
#Prior
plot(theta,dbeta(theta,8,4),type="l")
#Posterior
lines(theta,dbeta(theta,41,11))
#posterior
plot(theta,dbeta(theta,41,11),type="l")
#prior (dashed line )
lines(theta,dbeta(theta,8,4),lty=2)
#posterior
plot(theta,dbeta(theta,41,11),type="l")
#prior (dashed line )
lines(theta,dbeta(theta,8,4),lty=2)
#likelihood (dotted line)
lines(theta,dbinom(33,size=40,p=theta),lty=3)
#posterior
plot(theta,dbeta(theta,41,11),type="l")
#prior (dashed line )
lines(theta,dbeta(theta,8,4),lty=2)
#Scaled likelihood (dotted line)
lines(theta,44*dbinom(33,size=40,p=theta),lty=3)
1-pbeta(.25,41,11)
1-pbeta(.5,41,11)
1-pbeta(.8,41,11)
qbeta(.025,41,11)
qbeta(.975,41,11)
#posterior
plot(theta,dbeta(theta,41,11),type="l")
#prior (dashed line )
lines(theta,dbeta(theta,8,4),lty=2)
#Scaled likelihood (dotted line)
lines(theta,44*dbinom(33,size=40,p=theta),lty=3)
32/(32+20)  # posterior mean
24/40       # MLE
#posterior (solid line)
plot(theta,dbeta(theta,32,20),type="l")
#prior  (dashed line)
lines(theta,dbeta(theta,8,4),lty=2)
#likelihood (dotted line)
lines(theta,44*dbinom(24,size=40,p=theta),lty=3)
1-pbeta(.25,32,20)
1-pbeta(.5,32,20)
1-pbeta(.8,32,20)
#posterior (solid line)
plot(theta,dbeta(theta,32,20),type="l")
#prior  (dashed line)
lines(theta,dbeta(theta,8,4),lty=2)
#likelihood (dotted line)
lines(theta,44*dbinom(24,size=40,p=theta),lty=3)
qbeta(.025,32,20)
qbeta(.975,32,20)
theta1=rbeta(1000,41,11)
theta2=rbeta(1000,32,20)
mean(theta1>theta2)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(BAS)
library(GGally)
data("UScrime")
UScrime[,-2]=log(UScrime[,-2])
crime.ZS=bas.lm(y ~ ., data=UScrime, prior="ZS-null", modelprior=uniform(), method="MCMC")
diagnostics(crime.ZS)
plot(crime.ZS, which=1, add.smooth = F)
plot(crime.ZS, which=2, add.smooth = F)
plot(crime.ZS, which=3)
plot(crime.ZS, which=4)
image(crime.ZS, rotate=F)
coef.ZS=coef(crime.ZS)
plot(coef.ZS, subset=c(5:6), ask = F)
#highest probability model:
HPM <- predict(crime.ZS, estimator = "HPM")
# show the indices of variables in the best model where 0 is the intercept
HPM$bestmodel
variable.names(HPM)
#median probability model:
MPM <- predict(crime.ZS, estimator = "MPM")
variable.names(MPM)
#best predictive model:
BPM <- predict(crime.ZS, estimator = "BPM")
variable.names(BPM)
#Bayesian Model Averaging
BMA<-predict(crime.ZS, estimator = "BMA")
variable.names(BMA)
#Let’s see how they compare:
GGally::ggpairs(data.frame(
HPM = as.vector(HPM$fit), # this used predict so we need to extract fitted values
MPM = as.vector(MPM$fit), # this used fitted
BPM = as.vector(BPM$fit), # this used fitted
BMA = as.vector(BMA$fit))) # this used predict
library(tidyverse)
library(statsr)
data(brfss)
credible_interval_app()
# Type your code for Exercise 1 here.
qnorm(c(0.025, 0.975), mean = 10, sd = 2.236)
# Type your code for the Exercise 2 here.
qbeta(c(0.05, 0.95), shape1 = 2, shape2 = 5)
# Type your code for the Exercise 3 here.
qgamma(c(0.005, 0.995), shape = 4, rate = 8)
table(brfss$sex)
n <- length(brfss$sex)
x <- sum(brfss$sex == "Female")
# Type your code for Question 7 here.
table(brfss$exercise)
# Type your code for Question 10 here.
# Type your code for Question 12 here.
# Type your code for the Exercise 6 here.
